{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "\n",
        "# Read the file\n",
        "df = pd.read_csv(\"/content/drive/MyDrive/Interview Analysis Sample  - Sheet1.csv\")  # for CSV"
      ],
      "metadata": {
        "id": "s5jsXq79abAT"
      },
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "awlWqOoAaEbr",
        "outputId": "a4ca645d-706d-415d-c1a9-36cd98d61a79"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.11/dist-packages/xgboost/core.py:158: UserWarning: [16:11:57] WARNING: /workspace/src/learner.cc:740: \n",
            "Parameters: { \"use_label_encoder\" } are not used.\n",
            "\n",
            "  warnings.warn(smsg, UserWarning)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[XGBoost Phase A]\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.33      0.20      0.25       259\n",
            "           1       0.69      0.82      0.75       577\n",
            "\n",
            "    accuracy                           0.63       836\n",
            "   macro avg       0.51      0.51      0.50       836\n",
            "weighted avg       0.58      0.63      0.60       836\n",
            "\n",
            "[LightGBM] [Info] Number of positive: 1318, number of negative: 630\n",
            "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.000169 seconds.\n",
            "You can set `force_col_wise=true` to remove the overhead.\n",
            "[LightGBM] [Info] Total Bins 314\n",
            "[LightGBM] [Info] Number of data points in the train set: 1948, number of used features: 5\n",
            "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.676591 -> initscore=0.738151\n",
            "[LightGBM] [Info] Start training from score 0.738151\n",
            "[LightGBM Phase A]\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.36      0.15      0.21       259\n",
            "           1       0.70      0.88      0.78       577\n",
            "\n",
            "    accuracy                           0.65       836\n",
            "   macro avg       0.53      0.51      0.49       836\n",
            "weighted avg       0.59      0.65      0.60       836\n",
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.11/dist-packages/xgboost/core.py:158: UserWarning: [16:11:58] WARNING: /workspace/src/learner.cc:740: \n",
            "Parameters: { \"use_label_encoder\" } are not used.\n",
            "\n",
            "  warnings.warn(smsg, UserWarning)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[XGBoost Phase B]\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.34      0.38      0.36       263\n",
            "           1       0.19      0.18      0.19       154\n",
            "           2       0.18      0.14      0.15       182\n",
            "           3       0.30      0.32      0.31       237\n",
            "\n",
            "    accuracy                           0.28       836\n",
            "   macro avg       0.25      0.26      0.25       836\n",
            "weighted avg       0.27      0.28      0.27       836\n",
            "\n",
            "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.000169 seconds.\n",
            "You can set `force_col_wise=true` to remove the overhead.\n",
            "[LightGBM] [Info] Total Bins 314\n",
            "[LightGBM] [Info] Number of data points in the train set: 1948, number of used features: 5\n",
            "[LightGBM] [Info] Start training from score -1.206371\n",
            "[LightGBM] [Info] Start training from score -1.600749\n",
            "[LightGBM] [Info] Start training from score -1.575622\n",
            "[LightGBM] [Info] Start training from score -1.230678\n",
            "[LightGBM Phase B]\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.37      0.38      0.38       263\n",
            "           1       0.21      0.20      0.21       154\n",
            "           2       0.20      0.16      0.18       182\n",
            "           3       0.30      0.34      0.32       237\n",
            "\n",
            "    accuracy                           0.29       836\n",
            "   macro avg       0.27      0.27      0.27       836\n",
            "weighted avg       0.29      0.29      0.29       836\n",
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.11/dist-packages/xgboost/core.py:158: UserWarning: [16:11:58] WARNING: /workspace/src/learner.cc:740: \n",
            "Parameters: { \"use_label_encoder\" } are not used.\n",
            "\n",
            "  warnings.warn(smsg, UserWarning)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[XGBoost Phase C - Full Features]\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.32      0.38      0.35       263\n",
            "           1       0.22      0.21      0.22       154\n",
            "           2       0.23      0.14      0.18       182\n",
            "           3       0.32      0.35      0.33       237\n",
            "\n",
            "    accuracy                           0.29       836\n",
            "   macro avg       0.27      0.27      0.27       836\n",
            "weighted avg       0.28      0.29      0.28       836\n",
            "\n",
            "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.000123 seconds.\n",
            "You can set `force_row_wise=true` to remove the overhead.\n",
            "And if memory is not enough, you can set `force_col_wise=true`.\n",
            "[LightGBM] [Info] Total Bins 1211\n",
            "[LightGBM] [Info] Number of data points in the train set: 1948, number of used features: 9\n",
            "[LightGBM] [Info] Start training from score -1.206371\n",
            "[LightGBM] [Info] Start training from score -1.600749\n",
            "[LightGBM] [Info] Start training from score -1.575622\n",
            "[LightGBM] [Info] Start training from score -1.230678\n",
            "[LightGBM Phase C - Full Features]\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.32      0.36      0.34       263\n",
            "           1       0.20      0.18      0.19       154\n",
            "           2       0.20      0.14      0.16       182\n",
            "           3       0.30      0.35      0.32       237\n",
            "\n",
            "    accuracy                           0.28       836\n",
            "   macro avg       0.25      0.26      0.25       836\n",
            "weighted avg       0.27      0.28      0.27       836\n",
            "\n"
          ]
        }
      ],
      "source": [
        "# interview_outcome_prediction/main.py\n",
        "# === IMPORTS === #\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.metrics import classification_report, confusion_matrix\n",
        "import xgboost as xgb\n",
        "import lightgbm as lgb\n",
        "\n",
        "# === LOAD AND CLEAN DATA === #\n",
        "# Read the raw interview data\n",
        "df.columns = df.columns.str.strip().str.lower().str.replace(\" \", \"_\")  # Clean column names\n",
        "\n",
        "# Convert time fields to datetime objects for processing\n",
        "df[\"start_time\"] = pd.to_datetime(df[\"start_time\"])\n",
        "df[\"end_time\"] = pd.to_datetime(df[\"end_time\"])\n",
        "\n",
        "# === FEATURE ENGINEERING === #\n",
        "# Aggregate session-level metadata like number of questions, unique topics, and panel count\n",
        "summary = df.groupby(\"session\").agg({\n",
        "    \"start_time\": \"first\",\n",
        "    \"end_time\": \"first\",\n",
        "    \"question\": \"count\",\n",
        "    \"topic\": pd.Series.nunique,\n",
        "    \"panel\": pd.Series.nunique\n",
        "}).reset_index()\n",
        "\n",
        "# Rename aggregated columns\n",
        "summary.columns = [\"session\", \"start_time\", \"end_time\", \"num_questions\", \"unique_topics\", \"panel_count\"]\n",
        "\n",
        "# Compute interview duration in minutes\n",
        "summary[\"duration_mins\"] = (summary[\"end_time\"] - summary[\"start_time\"]).dt.total_seconds() / 60\n",
        "\n",
        "# Calculate average question length per session\n",
        "df[\"question_length\"] = df[\"question\"].apply(lambda x: len(str(x).split()))\n",
        "avg_len = df.groupby(\"session\")[\"question_length\"].mean().reset_index(name=\"avg_question_length\")\n",
        "summary = summary.merge(avg_len, on=\"session\")\n",
        "\n",
        "# === SIMULATE LABELS FOR PHASE A === #\n",
        "# Simulate verdicts randomly for binary classification\n",
        "np.random.seed(42)\n",
        "verdicts = [\"Yes\", \"WeakYes\", \"No\"]\n",
        "summary[\"verdict\"] = np.random.choice(verdicts, size=len(summary))\n",
        "summary[\"target\"] = summary[\"verdict\"].apply(lambda x: 0 if x == \"No\" else 1)\n",
        "\n",
        "# === PHASE A - BINARY CLASSIFICATION === #\n",
        "# Prepare features and target\n",
        "y = summary[\"target\"]\n",
        "X = summary[[\"num_questions\", \"unique_topics\", \"panel_count\", \"duration_mins\", \"avg_question_length\"]]\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=42)\n",
        "\n",
        "# Train XGBoost for binary classification\n",
        "model_xgb = xgb.XGBClassifier(use_label_encoder=False, eval_metric='logloss')\n",
        "model_xgb.fit(X_train, y_train)\n",
        "print(\"[XGBoost Phase A]\")\n",
        "print(classification_report(y_test, model_xgb.predict(X_test)))\n",
        "\n",
        "# Train LightGBM for binary classification\n",
        "model_lgb = lgb.LGBMClassifier()\n",
        "model_lgb.fit(X_train, y_train)\n",
        "print(\"[LightGBM Phase A]\")\n",
        "print(classification_report(y_test, model_lgb.predict(X_test)))\n",
        "\n",
        "# === PHASE B - MULTICLASS STAGE OUTCOME === #\n",
        "# Simulate outcome stage for multiclass prediction (e.g. R1 reject, R2, R3, Accept)\n",
        "summary[\"stage_outcome\"] = np.random.choice([0, 1, 2, 3], size=len(summary), p=[0.3, 0.2, 0.2, 0.3])\n",
        "y_multi = summary[\"stage_outcome\"]\n",
        "X_train_m, X_test_m, y_train_m, y_test_m = train_test_split(X, y_multi, test_size=0.3, random_state=42)\n",
        "\n",
        "# XGBoost multiclass classification\n",
        "model_xgb_m = xgb.XGBClassifier(objective='multi:softprob', num_class=4, use_label_encoder=False, eval_metric='mlogloss')\n",
        "model_xgb_m.fit(X_train_m, y_train_m)\n",
        "print(\"[XGBoost Phase B]\")\n",
        "print(classification_report(y_test_m, model_xgb_m.predict(X_test_m)))\n",
        "\n",
        "# LightGBM multiclass classification\n",
        "model_lgb_m = lgb.LGBMClassifier(objective='multiclass', num_class=4)\n",
        "model_lgb_m.fit(X_train_m, y_train_m)\n",
        "print(\"[LightGBM Phase B]\")\n",
        "print(classification_report(y_test_m, model_lgb_m.predict(X_test_m)))\n",
        "\n",
        "# === PHASE C SIMULATION: PANELIST STRICTNESS + INTERVIEW ADJUSTMENT === #\n",
        "# Simulate strictness for each panelist (based on prior behavior if available)\n",
        "panel_strictness = df.groupby(\"panel\").size().reset_index(name=\"interview_count\")\n",
        "panel_strictness[\"strictness\"] = np.clip(np.random.normal(0.6, 0.2, len(panel_strictness)), 0, 1)\n",
        "\n",
        "# Determine main panelist per session (for merging strictness score)\n",
        "def safe_mode(series):\n",
        "    mode_vals = series.mode()\n",
        "    return mode_vals[0] if not mode_vals.empty else np.nan\n",
        "\n",
        "top_panels = df.groupby(\"session\")[\"panel\"].agg(safe_mode).reset_index(name=\"main_panel\")\n",
        "\n",
        "# === PHASE C EXTENSION: SIMULATE CANDIDATE BEHAVIOR === #\n",
        "# Simulate interviewee behavior metrics: confidence, speed, correctness\n",
        "np.random.seed(99)\n",
        "summary[\"confidence_score\"] = np.clip(np.random.normal(loc=0.7, scale=0.15, size=len(summary)), 0, 1)\n",
        "summary[\"answer_speed\"] = np.clip(np.random.normal(loc=30, scale=10, size=len(summary)), 5, 90)\n",
        "summary[\"correctness_prob\"] = np.clip(np.random.normal(loc=0.65, scale=0.2, size=len(summary)), 0, 1)\n",
        "\n",
        "# Merge strictness score with session-level summary\n",
        "summary = summary.merge(top_panels.merge(panel_strictness, left_on=\"main_panel\", right_on=\"panel\", how=\"left\")[[\"session\", \"strictness\"]], on=\"session\", how=\"left\")\n",
        "\n",
        "# === FINAL PHASE C MODEL TRAINING WITH PANELIST + CANDIDATE FEATURES === #\n",
        "X_phase_c = summary[[\n",
        "    \"num_questions\", \"unique_topics\", \"panel_count\", \"duration_mins\",\n",
        "    \"avg_question_length\", \"strictness\", \"confidence_score\", \"answer_speed\", \"correctness_prob\"\n",
        "]]\n",
        "y_phase_c = summary[\"stage_outcome\"]\n",
        "X_train_c, X_test_c, y_train_c, y_test_c = train_test_split(X_phase_c, y_phase_c, test_size=0.3, random_state=42)\n",
        "\n",
        "# XGBoost Phase C model\n",
        "model_xgb_c = xgb.XGBClassifier(objective='multi:softprob', num_class=4, use_label_encoder=False, eval_metric='mlogloss')\n",
        "model_xgb_c.fit(X_train_c, y_train_c)\n",
        "print(\"[XGBoost Phase C - Full Features]\")\n",
        "print(classification_report(y_test_c, model_xgb_c.predict(X_test_c)))\n",
        "\n",
        "# LightGBM Phase C model\n",
        "model_lgb_c = lgb.LGBMClassifier(objective='multiclass', num_class=4)\n",
        "model_lgb_c.fit(X_train_c, y_train_c)\n",
        "print(\"[LightGBM Phase C - Full Features]\")\n",
        "print(classification_report(y_test_c, model_lgb_c.predict(X_test_c)))"
      ]
    }
  ]
}